{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe50Gsy0nRyILviH+KAcmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanskaar1/Sanskaar-Singh/blob/master/LunarLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t5YERojhLb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "35066dca-d102-4d66-f739-44e5193ce7a2"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "!pip install pyglet==1.5.11\n",
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "env = gym.make(\"LunarLander-v2\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n",
            "Collecting pyglet==1.5.11\n",
            "  Downloading pyglet-1.5.11-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "  Attempting uninstall: pyglet\n",
            "    Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires pyglet<=1.5.0,>=1.4.0, but you have pyglet 1.5.11 which is incompatible.\u001b[0m\n",
            "Successfully installed pyglet-1.5.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyglet"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xM4dhdWhRoV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.activations import relu, linear\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D-RTFiIhYHR"
      },
      "source": [
        "class DQN:\n",
        "\tdef __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
        "\n",
        "\t\tself.env = env\n",
        "\t\tself.action_space = env.action_space\n",
        "\t\tself.observation_space = env.observation_space\n",
        "\t\tself.counter = 0\n",
        "\n",
        "\t\tself.lr = lr\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.epsilon = epsilon\n",
        "\t\tself.epsilon_decay = epsilon_decay\n",
        "\t\tself.rewards_list = []\n",
        "\n",
        "\t\tself.replay_memory_buffer = deque(maxlen=500000)\n",
        "\t\tself.batch_size = 64\n",
        "\t\tself.epsilon_min = 0.01\n",
        "\t\tself.num_action_space = self.action_space.n\n",
        "\t\tself.num_observation_space = env.observation_space.shape[0]\n",
        "\t\tself.model = self.initialize_model()\n",
        "\n",
        "\tdef initialize_model(self):\n",
        "\t\tmodel = Sequential()\n",
        "\t\tmodel.add(Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
        "\t\tmodel.add(Dense(256, activation=relu))\n",
        "\t\tmodel.add(Dense(self.num_action_space, activation=linear))\n",
        "\n",
        "\t\t# Compile the model\n",
        "\t\tmodel.compile(loss=mean_squared_error,optimizer=Adam(lr=self.lr))\n",
        "\t\tprint(model.summary())\n",
        "\t\treturn model\n",
        "\n",
        "\tdef get_action(self, state):\n",
        "\t\tif np.random.rand() < self.epsilon:\n",
        "\t\t\treturn random.randrange(self.num_action_space)\n",
        "\n",
        "\t\tpredicted_actions = self.model.predict(state)\n",
        "\t\treturn np.argmax(predicted_actions[0])\n",
        "\n",
        "\tdef add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "\t\tself.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "\tdef learn_and_update_weights_by_reply(self):\n",
        "\n",
        "\t\t# replay_memory_buffer size check\n",
        "\t\tif len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
        "\t\t\treturn\n",
        "\n",
        "\t\t# Early Stopping\n",
        "\t\tif np.mean(self.rewards_list[-10:]) > 180:\n",
        "\t\t\treturn\n",
        "\n",
        "\t\trandom_sample = self.get_random_sample_from_replay_mem()\n",
        "\t\tstates, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
        "\t\ttargets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
        "\t\ttarget_vec = self.model.predict_on_batch(states)\n",
        "\t\tindexes = np.array([i for i in range(self.batch_size)])\n",
        "\t\ttarget_vec[[indexes], [actions]] = targets\n",
        "\n",
        "\t\tself.model.fit(states, target_vec, epochs=1, verbose=0)\n",
        "\n",
        "\tdef get_attribues_from_sample(self, random_sample):\n",
        "\t\tstates = np.array([i[0] for i in random_sample])\n",
        "\t\tactions = np.array([i[1] for i in random_sample])\n",
        "\t\trewards = np.array([i[2] for i in random_sample])\n",
        "\t\tnext_states = np.array([i[3] for i in random_sample])\n",
        "\t\tdone_list = np.array([i[4] for i in random_sample])\n",
        "\t\tstates = np.squeeze(states)\n",
        "\t\tnext_states = np.squeeze(next_states)\n",
        "\t\treturn np.squeeze(states), actions, rewards, next_states, done_list\n",
        "\n",
        "\tdef get_random_sample_from_replay_mem(self):\n",
        "\t\trandom_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "\t\treturn random_sample"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "nXsKUJUCXKbs",
        "outputId": "a0c7be6e-3863-4d3d-d939-c4c1be01dc50"
      },
      "source": [
        "\tdef train(self, num_episodes=2000, can_stop=True):\n",
        "\t\tfor episode in range(num_episodes):\n",
        "\t\t\tstate = env.reset()\n",
        "\t\t\treward_for_episode = 0\n",
        "\t\t\tnum_steps = 1000\n",
        "\t\t\tstate = np.reshape(state, [1, self.num_observation_space])\n",
        "\t\t\tfor step in range(num_steps):\n",
        "\t\t\t   env.render()\n",
        "received_action = self.get_action(state)\n",
        "\t\t\t\t# print(\"received_action:\", received_action)\n",
        "next_state, reward, done, info = env.step(received_action)\n",
        "next_state = np.reshape(next_state, [1, self.num_observation_space])\n",
        "\t\t\t\t# Store the experience in replay memory\n",
        "self.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
        "\t\t\t\t# add up rewards\n",
        "reward_for_episode += reward\n",
        "state = next_state\n",
        "self.update_counter()\n",
        "self.learn_and_update_weights_by_reply()\n",
        "\n",
        "if done:\n",
        "\t\t\t\t\tbreak\n",
        "self.rewards_list.append(reward_for_episode)\n",
        "\n",
        "\t\t\t# Decay the epsilon after each experience completion\n",
        "if self.epsilon > self.epsilon_min:\n",
        "\t\t\t\tself.epsilon *= self.epsilon_decay\n",
        "\n",
        "\t\t\t# Check for breaking condition\n",
        "last_rewards_mean = np.mean(self.rewards_list[-100:])\n",
        "if last_rewards_mean > 200 and can_stop:\n",
        "\t\t\t\tprint(\"DQN Training Complete...\")\n",
        "\t\t\t\tbreak\n",
        "print(episode, \"\\t: Episode || Reward: \",reward_for_episode, \"\\t|| Average Reward: \",last_rewards_mean, \"\\t epsilon: \", self.epsilon )\n",
        "\n",
        "def update_counter(self):\n",
        "\t\tself.counter += 1\n",
        "\t\tstep_size = 5\n",
        "\t\tself.counter = self.counter % step_size\n",
        "\n",
        "def save(self, name):\n",
        "\t\tself.model.save(name)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-84a6b1989863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mreceived_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                         \u001b[0;31m# print(\"received_action:\", received_action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreceived_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "HBqqEdiFXiat",
        "outputId": "342f375e-ccae-4eb6-8d48-2a7efc9d9969"
      },
      "source": [
        "def test_already_trained_model(trained_model):\n",
        "\trewards_list = []\n",
        "\tnum_test_episode = 100\n",
        "\tenv = gym.make(\"LunarLander-v2\")\n",
        "\tprint(\"Starting Testing of the trained model...\")\n",
        "\n",
        "\tstep_count = 1000\n",
        "\n",
        "\tfor test_episode in range(num_test_episode):\n",
        "\t\tcurrent_state = env.reset()\n",
        "\t\tnum_observation_space = env.observation_space.shape[0]\n",
        "\t\tcurrent_state = np.reshape(current_state, [1, num_observation_space])\n",
        "\t\treward_for_episode = 0\n",
        "\t\tfor step in range(step_count):\n",
        "\t\t\tenv.render()\n",
        "\t\t\tselected_action = np.argmax(trained_model.predict(current_state)[0])\n",
        "\t\t\tnew_state, reward, done, info = env.step(selected_action)\n",
        "\t\t\tnew_state = np.reshape(new_state, [1, num_observation_space])\n",
        "\t\t\tcurrent_state = new_state\n",
        "\t\t\treward_for_episode += reward\n",
        "\t\t\tif done:\n",
        "\t\t\t\tbreak\n",
        "\t\trewards_list.append(reward_for_episode)\n",
        "\t\tprint(test_episode, \"\\t: Episode || Reward: \", reward_for_episode)\n",
        "\n",
        "\treturn rewards_list\n",
        "\n",
        "\n",
        "def plot_df(df, chart_name, title, x_axis_label, y_axis_label):\n",
        "\tplt.rcParams.update({'font.size': 17})\n",
        "\tdf['rolling_mean'] = df[df.columns[0]].rolling(100).mean()\n",
        "\tplt.figure(figsize=(15, 8))\n",
        "\tplt.close()\n",
        "\tplt.figure()\n",
        "\t# plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n",
        "\tplot = df.plot(linewidth=1.5, figsize=(15, 8))\n",
        "\tplot.set_xlabel(x_axis_label)\n",
        "\tplot.set_ylabel(y_axis_label)\n",
        "\t# plt.ylim((-400, 300))\n",
        "\tfig = plot.get_figure()\n",
        "\tplt.legend().set_visible(False)\n",
        "\tfig.savefig(chart_name)\n",
        "\n",
        "\n",
        "def plot_df2(df, chart_name, title, x_axis_label, y_axis_label):\n",
        "\tdf['mean'] = df[df.columns[0]].mean()\n",
        "\tplt.rcParams.update({'font.size': 17})\n",
        "\tplt.figure(figsize=(15, 8))\n",
        "\tplt.close()\n",
        "\tplt.figure()\n",
        "\t# plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n",
        "\tplot = df.plot(linewidth=1.5, figsize=(15, 8))\n",
        "\tplot.set_xlabel(x_axis_label)\n",
        "\tplot.set_ylabel(y_axis_label)\n",
        "\tplt.ylim((0, 300))\n",
        "\tplt.xlim((0, 100))\n",
        "\tplt.legend().set_visible(False)\n",
        "\tfig = plot.get_figure()\n",
        "\tfig.savefig(chart_name)\n",
        "\n",
        "\n",
        "def plot_experiments(df, chart_name, title, x_axis_label, y_axis_label, y_limit):\n",
        "\tplt.rcParams.update({'font.size': 17})\n",
        "\tplt.figure(figsize=(15, 8))\n",
        "\tplt.close()\n",
        "\tplt.figure()\n",
        "\tplot = df.plot(linewidth=1, figsize=(15, 8), title=title)\n",
        "\tplot.set_xlabel(x_axis_label)\n",
        "\tplot.set_ylabel(y_axis_label)\n",
        "\tplt.ylim(y_limit)\n",
        "\tfig = plot.get_figure()\n",
        "\tfig.savefig(chart_name)\n",
        "\n",
        "\n",
        "def run_experiment_for_gamma():\n",
        "\tprint('Running Experiment for gamma...')\n",
        "\tenv = gym.make('LunarLander-v2')\n",
        "\n",
        "\t# set seeds\n",
        "\tenv.seed(21)\n",
        "\tnp.random.seed(21)\n",
        "\n",
        "\t# setting up params\n",
        "\tlr = 0.001\n",
        "\tepsilon = 1.0\n",
        "\tepsilon_decay = 0.995\n",
        "\tgamma_list = [0.99, 0.9, 0.8, 0.7]\n",
        "\ttraining_episodes = 1000\n",
        "\n",
        "\trewards_list_for_gammas = []\n",
        "\tfor gamma_value in gamma_list:\n",
        "\t\t# save_dir = \"hp_gamma_\"+ str(gamma_value) + \"_\"\n",
        "\t\tmodel = DQN(env, lr, gamma_value, epsilon, epsilon_decay)\n",
        "\t\tprint(\"Training model for Gamma: {}\".format(gamma_value))\n",
        "\t\tmodel.train(training_episodes, False)\n",
        "\t\trewards_list_for_gammas.append(model.rewards_list)\n",
        "\n",
        "\tpickle.dump(rewards_list_for_gammas, open(\"rewards_list_for_gammas.p\", \"wb\"))\n",
        "\trewards_list_for_gammas = pickle.load(open(\"rewards_list_for_gammas.p\", \"rb\"))\n",
        "\n",
        "\tgamma_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n",
        "\tfor i in range(len(gamma_list)):\n",
        "\t\tcol_name = \"gamma=\" + str(gamma_list[i])\n",
        "\t\tgamma_rewards_pd[col_name] = rewards_list_for_gammas[i]\n",
        "\tplot_experiments(gamma_rewards_pd, \"Figure 4: Rewards per episode for different gamma values\",\n",
        "\t\t\t\t\t \"Figure 4: Rewards per episode for different gamma values\", \"Episodes\", \"Reward\", (-600, 300))\n",
        "\n",
        "\n",
        "def run_experiment_for_lr():\n",
        "\tprint('Running Experiment for learning rate...')\n",
        "\tenv = gym.make('LunarLander-v2')\n",
        "\n",
        "\t# set seeds\n",
        "\tenv.seed(21)\n",
        "\tnp.random.seed(21)\n",
        "\n",
        "\t# setting up params\n",
        "\tlr_values = [0.0001, 0.001, 0.01, 0.1]\n",
        "\tepsilon = 1.0\n",
        "\tepsilon_decay = 0.995\n",
        "\tgamma = 0.99\n",
        "\ttraining_episodes = 1000\n",
        "\trewards_list_for_lrs = []\n",
        "\tfor lr_value in lr_values:\n",
        "\t\tmodel = DQN(env, lr_value, gamma, epsilon, epsilon_decay)\n",
        "\t\tprint(\"Training model for LR: {}\".format(lr_value))\n",
        "\t\tmodel.train(training_episodes, False)\n",
        "\t\trewards_list_for_lrs.append(model.rewards_list)\n",
        "\n",
        "\tpickle.dump(rewards_list_for_lrs, open(\"rewards_list_for_lrs.p\", \"wb\"))\n",
        "\trewards_list_for_lrs = pickle.load(open(\"rewards_list_for_lrs.p\", \"rb\"))\n",
        "\n",
        "\tlr_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n",
        "\tfor i in range(len(lr_values)):\n",
        "\t\tcol_name = \"lr=\"+ str(lr_values[i])\n",
        "\t\tlr_rewards_pd[col_name] = rewards_list_for_lrs[i]\n",
        "\tplot_experiments(lr_rewards_pd, \"Figure 3: Rewards per episode for different learning rates\", \"Figure 3: Rewards per episode for different learning rates\", \"Episodes\", \"Reward\", (-2000, 300))\n",
        "\n",
        "\n",
        "def run_experiment_for_ed():\n",
        "\tprint('Running Experiment for epsilon decay...')\n",
        "\tenv = gym.make('LunarLander-v2')\n",
        "\n",
        "\t# set seeds\n",
        "\tenv.seed(21)\n",
        "\tnp.random.seed(21)\n",
        "\n",
        "\t# setting up params\n",
        "\tlr = 0.001\n",
        "\tepsilon = 1.0\n",
        "\ted_values = [0.999, 0.995, 0.990, 0.9]\n",
        "\tgamma = 0.99\n",
        "\ttraining_episodes = 1000\n",
        "\n",
        "\trewards_list_for_ed = []\n",
        "\tfor ed in ed_values:\n",
        "\t\tsave_dir = \"hp_ed_\"+ str(ed) + \"_\"\n",
        "\t\tmodel = DQN(env, lr, gamma, epsilon, ed)\n",
        "\t\tprint(\"Training model for ED: {}\".format(ed))\n",
        "\t\tmodel.train(training_episodes, False)\n",
        "\t\trewards_list_for_ed.append(model.rewards_list)\n",
        "\n",
        "\tpickle.dump(rewards_list_for_ed, open(\"rewards_list_for_ed.p\", \"wb\"))\n",
        "\trewards_list_for_ed = pickle.load(open(\"rewards_list_for_ed.p\", \"rb\"))\n",
        "\n",
        "\ted_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes+1)))\n",
        "\tfor i in range(len(ed_values)):\n",
        "\t\tcol_name = \"epsilon_decay = \"+ str(ed_values[i])\n",
        "\t\ted_rewards_pd[col_name] = rewards_list_for_ed[i]\n",
        "\tplot_experiments(ed_rewards_pd, \"Figure 5: Rewards per episode for different epsilon(ε) decay\", \"Figure 5: Rewards per episode for different epsilon(ε) decay values\", \"Episodes\", \"Reward\", (-600, 300))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tenv = gym.make('LunarLander-v2')\n",
        "\n",
        "\t# set seeds\n",
        "\tenv.seed(21)\n",
        "\tnp.random.seed(21)\n",
        "\n",
        "\t# setting up params\n",
        "\tlr = 0.001\n",
        "\tepsilon = 1.0\n",
        "\tepsilon_decay = 0.995\n",
        "\tgamma = 0.99\n",
        "\ttraining_episodes = 2000\n",
        "\tprint('St')\n",
        "\tmodel = DQN(env, lr, gamma, epsilon, epsilon_decay)\n",
        "\tmodel.train(training_episodes, True)\n",
        "\n",
        "\t# Save Everything\n",
        "\tsave_dir = \"saved_models\"\n",
        "\t# Save trained model\n",
        "\tmodel.save(save_dir + \"trained_model.h5\")\n",
        "\n",
        "\t# Save Rewards list\n",
        "\tpickle.dump(model.rewards_list, open(save_dir + \"train_rewards_list.p\", \"wb\"))\n",
        "\trewards_list = pickle.load(open(save_dir + \"train_rewards_list.p\", \"rb\"))\n",
        "\n",
        "\t# plot reward in graph\n",
        "\treward_df = pd.DataFrame(rewards_list)\n",
        "\tplot_df(reward_df, \"Figure 1: Reward for each training episode\", \"Reward for each training episode\", \"Episode\",\"Reward\")\n",
        "\n",
        "\t# Test the model\n",
        "\ttrained_model = load_model(save_dir + \"trained_model.h5\")\n",
        "\ttest_rewards = test_already_trained_model(trained_model)\n",
        "\tpickle.dump(test_rewards, open(save_dir + \"test_rewards.p\", \"wb\"))\n",
        "\ttest_rewards = pickle.load(open(save_dir + \"test_rewards.p\", \"rb\"))\n",
        "\n",
        "\tplot_df2(pd.DataFrame(test_rewards), \"Figure 2: Reward for each testing episode\",\"Reward for each testing episode\", \"Episode\", \"Reward\")\n",
        "\tprint(\"Training and Testing Completed...!\")\n",
        "\n",
        "\t# Run experiments for hyper-parameter\n",
        "\trun_experiment_for_lr()\n",
        "\trun_experiment_for_ed()\n",
        "\trun_experiment_for_gamma()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e186a8fbec56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# set seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ]
    }
  ]
}